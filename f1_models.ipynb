{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decisions to make\n",
    "### 1. \n",
    "1. \n",
    "```\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "y = data['season_end_top_10']\n",
    "```\n",
    "2. \n",
    "```\n",
    "data['top_10_finish'] = np.where(data['podium'] <= 10, 1, 0)\n",
    "y = data['top_10_finish']\n",
    "```\n",
    "3. \n",
    "```\n",
    "y = data['podium'] \n",
    "```\n",
    "Key Differenced: \n",
    "* The first method determines if a driver finished in the top 10 in terms of points at the end of a season.\n",
    "* The first method uses group-wise operation (grouping by 'season'), while the second line applies a condition to each row independently.\n",
    "* The second method simply marks whether a driver finished in the top 10 in a particular event (not considering the entire season).\n",
    "* The third method is just something I tried, seem pretty bad.\n",
    "\n",
    "From the looks of it, method 1 makes best predictions. Gets all first 3 places correct with RL and NBC.\n",
    "\n",
    "### 2. \n",
    "What kind of accuracy do we want to calculate?\n",
    "1. In top 10 (True/False)\n",
    "2. By standing\n",
    "\n",
    "Currently, I am computing the accuracy with top 10 seasonal standing, i.e., final standing, by `driver_name`.\n",
    "```\n",
    "accuracy = calc_acc(y_test, y_pred)\n",
    "```\n",
    "where `y_pred` is computed by:\n",
    "1. predicting the ranking of each round of a season\n",
    "2. assign points to the top 10\n",
    "3. sum all points by driver\n",
    "4. return driver_name of the top 10 drivers with most point in sorted order\n",
    "\n",
    "and `y_test` is computed in similar fasion but using real data.\n",
    "\n",
    "I personally think this accuracy isn't that meaningful because we are only comparing a single season. \n",
    "\n",
    "If we were to predict the seasonal top 10 for every season, this method of computing accuracy would make more sense.\n",
    "\n",
    "The NDCG score is quite interesting because it evaluates the ranking even if they do not match exactly (unlike accuracy). \n",
    "\n",
    "### 3. \n",
    "What feature do we want to include for training?\n",
    "1. With weather\n",
    "2. Without weather\n",
    "3. All features\n",
    "\n",
    "I've tried the feature set that Nishtha/Daniel provided (with weather) and the feature set that Joan/Mayesha has provided (without weather) and it seems like the model performed better without weather.\n",
    "\n",
    "During training, I ran into warnings `have not converged after reaching max_iter` for both LR and NN. Maybe less features is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions of Standing for Each Round with Chosen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "def calculate_dcg(relevances, p):\n",
    "    dcg = 0\n",
    "    for i in range(p):\n",
    "        dcg += (2 ** relevances[i] - 1) / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def calculate_ndcg(relevances, p):\n",
    "    dcg = calculate_dcg(relevances, p)\n",
    "    idcg = calculate_dcg(sorted(relevances, reverse=True), p)\n",
    "    return dcg / idcg if idcg != 0 else 0\n",
    "\n",
    "def map_true_relevances_to_predicted_order(true_relevances, predicted_relevances):\n",
    "    print(f\"true_relevances: {true_relevances}\")\n",
    "    print(f\"predicted_relevances: {predicted_relevances}\")\n",
    "    sorted_indices = np.argsort(predicted_relevances)[::-1]\n",
    "    print(f\"sorted_indices: {sorted_indices}\")\n",
    "\n",
    "    mapped_true_relevances = np.array(true_relevances)[sorted_indices]\n",
    "    print(f\"mapped_true_relevances: {mapped_true_relevances}\")\n",
    "    return mapped_true_relevances\n",
    "\n",
    "#Finding season winners based on points\n",
    "points_system = {1: 25, 2: 18, 3: 15, 4: 12, 5: 10, 6: 8, 7: 6, 8: 4, 9: 2, 10: 1}\n",
    "# Function to assign points based on predicted position\n",
    "def assign_points(df):\n",
    "    df['predicted_points'] = df.groupby('round').cumcount() + 1\n",
    "    df['predicted_points'] = df['predicted_points'].map(points_system).fillna(0)\n",
    "    return df\n",
    "\n",
    "def assign_relevance(values):\n",
    "    # Sorting the list in ascending order\n",
    "    sorted_values = sorted(values)\n",
    "    # Assigning points: highest (10) to lowest (1)\n",
    "    points = {value: 10 - i for i, value in enumerate(sorted_values)}\n",
    "    rel = [points[val] for val in values]\n",
    "    return rel\n",
    "\n",
    "def lr(X_train, y_train, X_test, y_test, output=False):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter=1500)\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # probabilities = neuralnet.predict_proba(X_test_scaled)[:, 1]\n",
    "    # data_2022['pred_probability'] = probabilities\n",
    "    # driver_probabilities = data_2022.groupby('driver_name')['pred_probability'].mean().sort_values(ascending=False)\n",
    "    # top_10_finishers = driver_probabilities.head(10)\n",
    "    # y_pred = neuralnet.predict(X_test_scaled)\n",
    "    \n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022.loc[:, 'pred_probability'] = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    top_10_predictions = data_2022.groupby('round').apply(lambda x: x.nlargest(10, 'pred_probability')).reset_index(drop=True)\n",
    "\n",
    "    unique_rounds = data_2022['round'].unique()\n",
    "\n",
    "    mapped_revs = []\n",
    "    ideal_revs = []\n",
    "\n",
    "    for r in unique_rounds:\n",
    "        round_predictions = top_10_predictions[top_10_predictions['round'] == r]\n",
    "        round_predictions = round_predictions[['season', 'round', 'driver_name', 'pred_probability', 'podium']]\n",
    "        round_predictions = round_predictions.reset_index(drop=True)\n",
    "        round_predictions['pred_podium'] = round_predictions.index + 1\n",
    "        \n",
    "        print(f\"Round {r} predictions:\")\n",
    "        display(round_predictions[['driver_name', 'podium', 'pred_podium']])\n",
    "        \n",
    "        podium_vals = round_predictions['podium'].tolist()\n",
    "        podium_vals = [int(val) for val in podium_vals]\n",
    "        true_relevances = assign_relevance(podium_vals)\n",
    "        predicted_relevances = assign_relevance(round_predictions['pred_podium'].tolist())\n",
    "        \n",
    "        # Map true relevances to the order of predicted relevances\n",
    "        mapped_true_relevances = map_true_relevances_to_predicted_order(true_relevances, predicted_relevances)\n",
    "        # Calculate NDCG for the mapped true relevances\n",
    "        ndcg_score_mapped = calculate_ndcg(mapped_true_relevances, len(mapped_true_relevances))\n",
    "        # Calculate ideal NDCG for true relevances\n",
    "        ideal_ndcg_score = calculate_ndcg(sorted(true_relevances, reverse=True), len(true_relevances))\n",
    "        mapped_revs.append(ndcg_score_mapped)\n",
    "        ideal_revs.append(ideal_ndcg_score)\n",
    "\n",
    "    top_10_predictions = assign_points(top_10_predictions)\n",
    "    # Summing up points for each driver\n",
    "    season_points = top_10_predictions.groupby('driver_name')['predicted_points'].sum().reset_index()\n",
    "    # Sorting drivers based on total points\n",
    "    season_points = season_points.sort_values('predicted_points', ascending=False)\n",
    "\n",
    "    y_test = y_test['driver_name'].tolist()\n",
    "    y_pred = season_points['driver_name'].head(10).tolist()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    comp_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "    if output: print(\"Logistic Regression\")\n",
    "    if output: display(comp_df)\n",
    "\n",
    "    pred_champ = season_points.iloc[0]['driver_name']\n",
    "    pred_champ_points = season_points.iloc[0]['predicted_points']\n",
    "\n",
    "    return (y_pred, accuracy, mapped_revs, ideal_revs, pred_champ, pred_champ_points)\n",
    "\n",
    "def nn(X_train, y_train, X_test, y_test, output=False):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    neuralnet = MLPClassifier(solver='sgd', batch_size= 100, random_state=0, max_iter=2000)\n",
    "    neuralnet.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # probabilities = neuralnet.predict_proba(X_test_scaled)[:, 1]\n",
    "    # data_2022['pred_probability'] = probabilities\n",
    "    # driver_probabilities = data_2022.groupby('driver_name')['pred_probability'].mean().sort_values(ascending=False)\n",
    "    # top_10_finishers = driver_probabilities.head(10)\n",
    "    # y_pred = neuralnet.predict(X_test_scaled)\n",
    "    \n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022.loc[:, 'pred_probability'] = neuralnet.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    top_10_predictions = data_2022.groupby('round').apply(lambda x: x.nlargest(10, 'pred_probability')).reset_index(drop=True)\n",
    "\n",
    "    unique_rounds = data_2022['round'].unique()\n",
    "\n",
    "    mapped_revs = []\n",
    "    ideal_revs = []\n",
    "\n",
    "    for r in unique_rounds:\n",
    "        round_predictions = top_10_predictions[top_10_predictions['round'] == r]\n",
    "        round_predictions = round_predictions[['season', 'round', 'driver_name', 'pred_probability', 'podium']]\n",
    "        round_predictions = round_predictions.reset_index(drop=True)\n",
    "        round_predictions['pred_podium'] = round_predictions.index + 1\n",
    "        \n",
    "        print(f\"Round {r} predictions:\")\n",
    "        display(round_predictions[['driver_name', 'podium', 'pred_podium']])\n",
    "\n",
    "        podium_vals = round_predictions['podium'].tolist()\n",
    "        podium_vals = [int(val) for val in podium_vals]\n",
    "        true_relevances = assign_relevance(podium_vals)\n",
    "        predicted_relevances = assign_relevance(round_predictions['pred_podium'].tolist())\n",
    "\n",
    "        # Map true relevances to the order of predicted relevances\n",
    "        mapped_true_relevances = map_true_relevances_to_predicted_order(true_relevances, predicted_relevances)\n",
    "        # Calculate NDCG for the mapped true relevances\n",
    "        ndcg_score_mapped = calculate_ndcg(mapped_true_relevances, len(mapped_true_relevances))\n",
    "        # Calculate ideal NDCG for true relevances\n",
    "        ideal_ndcg_score = calculate_ndcg(sorted(true_relevances, reverse=True), len(true_relevances))\n",
    "        mapped_revs.append(ndcg_score_mapped)\n",
    "        ideal_revs.append(ideal_ndcg_score)\n",
    "\n",
    "    top_10_predictions = assign_points(top_10_predictions)\n",
    "    # Summing up points for each driver\n",
    "    season_points = top_10_predictions.groupby('driver_name')['predicted_points'].sum().reset_index()\n",
    "    # Sorting drivers based on total points\n",
    "    season_points = season_points.sort_values('predicted_points', ascending=False)\n",
    "\n",
    "    y_test = y_test['driver_name'].tolist()\n",
    "    y_pred = season_points['driver_name'].head(10).tolist()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    comp_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "    if output: print(\"Neural Network\")\n",
    "    if output: display(comp_df)\n",
    "\n",
    "    pred_champ = season_points.iloc[0]['driver_name']\n",
    "    pred_champ_points = season_points.iloc[0]['predicted_points']\n",
    "\n",
    "    return (y_pred, accuracy, mapped_revs, ideal_revs, pred_champ, pred_champ_points)\n",
    "\n",
    "def nbc(X_train, y_train, X_test, y_test, output=False):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    nbc = GaussianNB()\n",
    "    nbc.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # probabilities = neuralnet.predict_proba(X_test_scaled)[:, 1]\n",
    "    # data_2022['pred_probability'] = probabilities\n",
    "    # driver_probabilities = data_2022.groupby('driver_name')['pred_probability'].mean().sort_values(ascending=False)\n",
    "    # top_10_finishers = driver_probabilities.head(10)\n",
    "    # y_pred = neuralnet.predict(X_test_scaled)\n",
    "    \n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022.loc[:, 'pred_probability'] = nbc.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    top_10_predictions = data_2022.groupby('round').apply(lambda x: x.nlargest(10, 'pred_probability')).reset_index(drop=True)\n",
    "\n",
    "    unique_rounds = data_2022['round'].unique()\n",
    "\n",
    "    mapped_revs = []\n",
    "    ideal_revs = []\n",
    "\n",
    "    for r in unique_rounds:\n",
    "        round_predictions = top_10_predictions[top_10_predictions['round'] == r]\n",
    "        round_predictions = round_predictions[['season', 'round', 'driver_name', 'pred_probability', 'podium']]\n",
    "        round_predictions = round_predictions.reset_index(drop=True)\n",
    "        round_predictions['pred_podium'] = round_predictions.index + 1\n",
    "        \n",
    "        print(f\"Round {r} predictions:\")\n",
    "        display(round_predictions[['driver_name', 'podium', 'pred_podium']])\n",
    "\n",
    "        podium_vals = round_predictions['podium'].tolist()\n",
    "        podium_vals = [int(val) for val in podium_vals]\n",
    "        true_relevances = assign_relevance(podium_vals)\n",
    "        predicted_relevances = assign_relevance(round_predictions['pred_podium'].tolist())\n",
    "\n",
    "        # Map true relevances to the order of predicted relevances\n",
    "        mapped_true_relevances = map_true_relevances_to_predicted_order(true_relevances, predicted_relevances)\n",
    "        # Calculate NDCG for the mapped true relevances\n",
    "        ndcg_score_mapped = calculate_ndcg(mapped_true_relevances, len(mapped_true_relevances))\n",
    "        # Calculate ideal NDCG for true relevances\n",
    "        ideal_ndcg_score = calculate_ndcg(sorted(true_relevances, reverse=True), len(true_relevances))\n",
    "        mapped_revs.append(ndcg_score_mapped)\n",
    "        ideal_revs.append(ideal_ndcg_score)\n",
    "\n",
    "    top_10_predictions = assign_points(top_10_predictions)\n",
    "    # Summing up points for each driver\n",
    "    season_points = top_10_predictions.groupby('driver_name')['predicted_points'].sum().reset_index()\n",
    "    # Sorting drivers based on total points\n",
    "    season_points = season_points.sort_values('predicted_points', ascending=False)\n",
    "\n",
    "    y_test = y_test['driver_name'].tolist()\n",
    "    y_pred = season_points['driver_name'].head(10).tolist()\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    comp_df = pd.DataFrame({'y_test': y_test, 'y_pred': y_pred})\n",
    "    if output: print(\"Naiive Bayes Classifier\")\n",
    "    if output: display(comp_df)\n",
    "\n",
    "    pred_champ = season_points.iloc[0]['driver_name']\n",
    "    pred_champ_points = season_points.iloc[0]['predicted_points']\n",
    "\n",
    "    return (y_pred, accuracy, mapped_revs, ideal_revs, pred_champ, pred_champ_points)\n",
    "\n",
    "# features = [\n",
    "#     'season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins', \n",
    "#     'constructor_points', 'constructor_standings_pos', 'constructor_wins',\n",
    "#     'fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3',\n",
    "#     'weather_cloudy', 'weather_cold', 'weather_dry', 'weather_warm', 'weather_wet'\n",
    "# ]\n",
    "features = ['season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins',\n",
    "            'constructor_points', 'constructor_standings_pos', 'constructor_wins']\n",
    "additional_features = ['fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3']\n",
    "features = features + additional_features\n",
    "\n",
    "data = pd.read_csv('./data/preprocessed_df.csv')\n",
    "# data['top_10_finish'] = np.where(data['podium'] <= 10, 1, 0)\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "\n",
    "X = data[features]\n",
    "# y = data['podium']\n",
    "# y = data['top_10_finish']\n",
    "y = data['season_end_top_10']\n",
    "\n",
    "X_train = X[data['season'] < 2022]\n",
    "y_train = y[data['season'] < 2022]\n",
    "X_test = X[data['season'] == 2022]\n",
    "\n",
    "data_w_points = pd.read_csv('./data/final_df.csv')\n",
    "y_test = data_w_points[data_w_points['season'] == 2022]\n",
    "y_test = y_test.groupby('driver_name')['points'].sum().reset_index()\n",
    "y_test = y_test.sort_values('points', ascending=False)\n",
    "y_test = y_test.iloc[0:10].reset_index(drop=True)\n",
    "\n",
    "(lr_pred, lr_acc, lr_mapped_revs, lr_ideal_revs, lr_champ, lr_champ_pts) = lr(X_train, y_train, X_test, y_test, output=True)\n",
    "(nn_pred, nn_acc, nn_mapped_revs, nn_ideal_revs, nn_champ, nn_champ_pts) = nn(X_train, y_train, X_test, y_test, output=True)\n",
    "(nbc_pred, nbc_acc, nbc_mapped_revs, nbc_ideal_revs, nbc_champ, nbc_champ_pts) = nbc(X_train, y_train, X_test, y_test, output=True)\n",
    "\n",
    "# print(\"Accuracy for 2022 season:\", nn_acc)\n",
    "# print(nn_pred)\n",
    "# print(f\"Champion of 2022 season: {champ} with {champ_pts} points\")\n",
    "lr_mean_mapped_revs = np.mean(lr_mapped_revs)\n",
    "nn_mean_mapped_revs = np.mean(nn_mapped_revs)\n",
    "nbc_mean_mapped_revs = np.mean(nbc_mapped_revs)\n",
    "\n",
    "results = {\"model\": [\"LR\", \"NN\", \"NBC\"], \"accuracy\": [lr_acc, nn_acc, nbc_acc], \"champion\": [lr_champ, nn_champ, nbc_champ], \"champion_points\": [lr_champ_pts, nn_champ_pts, nbc_champ_pts], \"mean_mapped_relevance\": [lr_mean_mapped_revs, nn_mean_mapped_revs, nbc_mean_mapped_revs]}\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "# display(y_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(lr_mapped_revs)+1), lr_mapped_revs, linestyle='--', marker='o', label='LR')\n",
    "plt.plot(range(1, len(nn_mapped_revs)+1), nn_mapped_revs, linestyle='--',marker='^', label='NN')\n",
    "plt.plot(range(1, len(nbc_mapped_revs)+1), nbc_mapped_revs, linestyle='--',marker='s', label='NBC')\n",
    "plt.xlabel('Round')\n",
    "plt.ylabel('NDCG Score')\n",
    "plt.legend()\n",
    "plt.title('Normalized Discounted Cumulative Gain per Round for Season 2022')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curves with Chosen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = [\n",
    "#     'season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins', \n",
    "#     'constructor_points', 'constructor_standings_pos', 'constructor_wins',\n",
    "#     'fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3',\n",
    "#     'weather_cloudy', 'weather_cold', 'weather_dry', 'weather_warm', 'weather_wet'\n",
    "# ]\n",
    "\n",
    "features = ['season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins',\n",
    "            'constructor_points', 'constructor_standings_pos', 'constructor_wins']\n",
    "additional_features = ['fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3']\n",
    "features = features + additional_features\n",
    "\n",
    "data = pd.read_csv('./data/preprocessed_df.csv')\n",
    "# data['top_10_finish'] = np.where(data['podium'] <= 10, 1, 0)\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "\n",
    "X = data[features]\n",
    "# y = data['podium']\n",
    "# y = data['top_10_finish']\n",
    "y = data['season_end_top_10']\n",
    "\n",
    "X_train = X[data['season'] < 2022]\n",
    "y_train = y[data['season'] < 2022]\n",
    "X_test = X[data['season'] == 2022]\n",
    "\n",
    "data_w_points = pd.read_csv('./data/final_df.csv')\n",
    "y_test = data_w_points[data_w_points['season'] == 2022]\n",
    "y_test = y_test.groupby('driver_name')['points'].sum().reset_index()\n",
    "y_test = y_test.sort_values('points', ascending=False)\n",
    "y_test = y_test.iloc[0:10].reset_index(drop=True)\n",
    "# y_test computes the actual final standing for each driver at the end of the season\n",
    "\n",
    "\n",
    "#  ===\n",
    "\n",
    "X_train_partitions = np.array_split(X_train, 10) # for different train sizes\n",
    "y_train_partitions = np.array_split(y_train, 10) # for different train sizes\n",
    "\n",
    "lr_accs = []\n",
    "nn_accs = []\n",
    "nbc_accs = []\n",
    "train_size = []\n",
    "X_train_var = pd.DataFrame()\n",
    "y_train_var = pd.DataFrame()\n",
    "for i in range(len(X_train_partitions)-1, 0, -1):\n",
    "    X_train_var = pd.concat([X_train_var, pd.DataFrame(X_train_partitions[i])])\n",
    "    y_train_var = pd.concat([y_train_var, pd.DataFrame(y_train_partitions[i])])\n",
    "    \n",
    "    X_train_var = X_train_var.sort_index()\n",
    "    # print(X_train_var.shape)\n",
    "    # display(X_train_var)\n",
    "\n",
    "    (lr_pred, lr_acc, lr_mapped_revs, lr_ideal_revs, lr_champ, lr_champ_pts) = lr(X_train_var, y_train_var, X_test, y_test)\n",
    "    (nn_pred, nn_acc, nn_mapped_revs, nn_ideal_revs, nn_champ, nn_champ_pts) = nn(X_train_var, y_train_var, X_test, y_test)\n",
    "    (nbc_pred, nbc_acc, nbc_mapped_revs, nbc_ideal_revs, nbc_champ, nbc_champ_pts) = nbc(X_train_var, y_train_var, X_test, y_test)\n",
    "    train_size.append(X_train_var.shape[0])\n",
    "    lr_accs.append(lr_acc)\n",
    "    nn_accs.append(nn_acc)\n",
    "    nbc_accs.append(nbc_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_size, lr_accs, linestyle='--', marker='o', label='LR')\n",
    "plt.plot(train_size, nn_accs, linestyle='--', marker='^', label='NN')\n",
    "plt.plot(train_size, nbc_accs, linestyle='--', marker='s', label='NBC')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Curves with Different Training Set Sizes')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ty_test\t        y_pred\n",
    "\n",
    "0\tmax verstappen\tmax verstappen  1\n",
    "\n",
    "1\tcharles leclerc\tcharles leclerc 1\n",
    "\n",
    "2\tsergio perez\tsergio perez    1\n",
    "\n",
    "3\tgeorge russell\tcarlos sainz    0\n",
    "\n",
    "4\tlewis hamilton\tgeorge russell  0\n",
    "\n",
    "5\tcarlos sainz\tlewis hamilton  0\n",
    "\n",
    "6\tlando norris\tlando norris    1\n",
    "\n",
    "7\testeban ocon\testeban ocon    1\n",
    "\n",
    "8\tfernando alonso\tfernando alonso 1\n",
    "\n",
    "9\tvaltteri bottas\tvaltteri bottas 1\n",
    "\n",
    "acc = 7/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original LR Untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./data/preprocessed_df.csv')\n",
    "data_w_points = pd.read_csv('./data/final_df.csv')\n",
    "\n",
    "def lr(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    probabilities = logreg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022['predicted_probability'] = probabilities\n",
    "\n",
    "    driver_probabilities = data_2022.groupby('driver_name')['predicted_probability'].mean().sort_values(ascending=False)\n",
    "    top_10_finishers = driver_probabilities.head(10)\n",
    "\n",
    "    y_pred = logreg.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return (y_pred, accuracy, top_10_finishers)\n",
    "\n",
    "features = [\n",
    "    'season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins', \n",
    "    'constructor_points', 'constructor_standings_pos', 'constructor_wins',\n",
    "    'fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3',\n",
    "    'weather_cloudy', 'weather_cold', 'weather_dry', 'weather_warm', 'weather_wet'\n",
    "]\n",
    "\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "\n",
    "X = data[features]\n",
    "y = data['season_end_top_10']\n",
    "\n",
    "X_train = X[data['season'] < 2022]\n",
    "y_train = y[data['season'] < 2022]\n",
    "X_test = X[data['season'] == 2022]\n",
    "y_test = y[data['season'] == 2022]\n",
    "\n",
    "X_train_partitions = np.array_split(X_train, 10) # for different train sizes\n",
    "y_train_partitions = np.array_split(y_train, 10) # for different train sizes\n",
    "\n",
    "lr_accs = []\n",
    "train_size = []\n",
    "X_train_var = pd.DataFrame()\n",
    "y_train_var = pd.DataFrame()\n",
    "for i in range(len(X_train_partitions)-1, 0, -1):\n",
    "    X_train_var = pd.concat([X_train_var, pd.DataFrame(X_train_partitions[i])])\n",
    "    y_train_var = pd.concat([y_train_var, pd.DataFrame(y_train_partitions[i])])\n",
    "    \n",
    "    X_train_var = X_train_var.sort_index()\n",
    "    # print(X_train_var.shape)\n",
    "    # display(X_train_var)\n",
    "\n",
    "    (lr_pred, lr_acc, lr_top10) = lr(X_train_var, y_train_var, X_test, y_test)\n",
    "    train_size.append(X_train_var.shape[0])\n",
    "    lr_accs.append(lr_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_size, lr_accs, marker='o', label='LR')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Curve of LR with Different Training Set Sizes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every driver, is it in the top 10 (true or false)\n",
    "\n",
    "For all rounds in season 2022, each driver has either true or false\n",
    "\n",
    "                    actual  pred\n",
    "\n",
    "2022, 1, driver1,   true,   true    1\n",
    "\n",
    "2022, 1, driver2,   false,  true    0\n",
    "\n",
    "2022, 1, driver3,   true,   false   0\n",
    "\n",
    "2022, 1, driver4,   false,  false   1\n",
    "\n",
    "2022, 1, driver5,   true,   false   0\n",
    "\n",
    "2022, 1, driver6,   false,  fasle   1\n",
    "\n",
    "acc = 3/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original NN Untouched\n",
    "**Notice that this implementation uses the second method!**\n",
    "```\n",
    "data['top_10_finish'] = np.where(data['podium'] <= 10, 1, 0)\n",
    "X = data[all_features]\n",
    "y = data['top_10_finish']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "data = pd.read_csv('./data/preprocessed_df.csv')\n",
    "\n",
    "def nn(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    neuralnet = MLPClassifier(solver='sgd', batch_size= 100, random_state=0, max_iter=1000)\n",
    "    neuralnet.fit(X_train_scaled, y_train)\n",
    "\n",
    "    probabilities = neuralnet.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022['predicted_probability'] = probabilities\n",
    "\n",
    "    driver_probabilities = data_2022.groupby('driver_name')['predicted_probability'].mean().sort_values(ascending=False)\n",
    "    top_10_finishers = driver_probabilities.head(10)\n",
    "\n",
    "    y_pred = neuralnet.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return (y_pred, accuracy, top_10_finishers)\n",
    "\n",
    "features = [\n",
    "    'season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins', \n",
    "    'constructor_points', 'constructor_standings_pos', 'constructor_wins',\n",
    "    'fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3',\n",
    "    'weather_cloudy', 'weather_cold', 'weather_dry', 'weather_warm', 'weather_wet'\n",
    "]\n",
    "\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "\n",
    "X = data[features]\n",
    "y = data['season_end_top_10']\n",
    "\n",
    "# X = data[all_features]\n",
    "# y = data['top_10_finish']\n",
    "\n",
    "X_train = X[data['season'] < 2022]\n",
    "y_train = y[data['season'] < 2022]\n",
    "X_test = X[data['season'] == 2022]\n",
    "y_test = y[data['season'] == 2022]\n",
    "\n",
    "X_train_partitions = np.array_split(X_train, 10) # for different train sizes\n",
    "y_train_partitions = np.array_split(y_train, 10) # for different train sizes\n",
    "\n",
    "nn_accs = []\n",
    "train_size = []\n",
    "X_train_var = pd.DataFrame()\n",
    "y_train_var = pd.DataFrame()\n",
    "for i in range(len(X_train_partitions)-1, 0, -1):\n",
    "    X_train_var = pd.concat([X_train_var, pd.DataFrame(X_train_partitions[i])])\n",
    "    y_train_var = pd.concat([y_train_var, pd.DataFrame(y_train_partitions[i])])\n",
    "    \n",
    "    X_train_var = X_train_var.sort_index()\n",
    "    # print(X_train_var.shape)\n",
    "    # display(X_train_var)\n",
    "\n",
    "    (nn_pred, nn_acc, nn_top10) = nn(X_train_var, y_train_var, X_test, y_test)\n",
    "    train_size.append(X_train_var.shape[0])\n",
    "    nn_accs.append(nn_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_size, nn_accs, marker='o', label='NN')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Curve of NN with Different Training Set Sizes')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original NBC Untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('./data/preprocessed_df.csv')\n",
    "data_w_points = pd.read_csv('./data/final_df.csv')\n",
    "\n",
    "def nbc(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    nbc = GaussianNB()\n",
    "    nbc.fit(X_train_scaled, y_train)\n",
    "\n",
    "    probabilities = nbc.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    data_2022 = data[data['season'] == 2022].copy()\n",
    "    data_2022['predicted_probability'] = probabilities\n",
    "\n",
    "    driver_probabilities = data_2022.groupby('driver_name')['predicted_probability'].mean().sort_values(ascending=False)\n",
    "    top_10_finishers = driver_probabilities.head(10)\n",
    "\n",
    "    y_pred = nbc.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return (y_pred, accuracy, top_10_finishers)\n",
    "\n",
    "features = [\n",
    "    'season', 'round', 'driver_points', 'driver_standings_pos', 'driver_wins', \n",
    "    'constructor_points', 'constructor_standings_pos', 'constructor_wins',\n",
    "    'fp_pos_1', 'fp_time_1', 'fp_pos_2', 'fp_time_2', 'fp_pos_3', 'fp_time_3',\n",
    "    'weather_cloudy', 'weather_cold', 'weather_dry', 'weather_warm', 'weather_wet'\n",
    "]\n",
    "\n",
    "data['season_end_top_10'] = data.groupby('season')['driver_points'].transform(lambda x: x >= x.nlargest(10).min())\n",
    "\n",
    "X = data[features]\n",
    "y = data['season_end_top_10']\n",
    "\n",
    "X_train = X[data['season'] < 2022]\n",
    "y_train = y[data['season'] < 2022]\n",
    "X_test = X[data['season'] == 2022]\n",
    "y_test = y[data['season'] == 2022]\n",
    "\n",
    "X_train_partitions = np.array_split(X_train, 10) # for different train sizes\n",
    "y_train_partitions = np.array_split(y_train, 10) # for different train sizes\n",
    "\n",
    "nbc_accs = []\n",
    "train_size = []\n",
    "X_train_var = pd.DataFrame()\n",
    "y_train_var = pd.DataFrame()\n",
    "for i in range(len(X_train_partitions)-1, 0, -1):\n",
    "    X_train_var = pd.concat([X_train_var, pd.DataFrame(X_train_partitions[i])])\n",
    "    y_train_var = pd.concat([y_train_var, pd.DataFrame(y_train_partitions[i])])\n",
    "    \n",
    "    X_train_var = X_train_var.sort_index()\n",
    "    # print(X_train_var.shape)\n",
    "    # display(X_train_var)\n",
    "\n",
    "    (nbc_pred, nbc_acc, nbc_top10) = nbc(X_train_var, y_train_var, X_test, y_test)\n",
    "    train_size.append(X_train_var.shape[0])\n",
    "    nbc_accs.append(nbc_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_size, lr_accs, marker='o', label='LR')\n",
    "plt.plot(train_size, nn_accs, marker='^', label='NN')\n",
    "plt.plot(train_size, nbc_accs, marker='s', label='NBC')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Learning Curve of LR with Different Training Set Sizes')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the \"Originals\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "573",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
